{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.12"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8378585,"sourceType":"datasetVersion","datasetId":4982232,"isSourceIdPinned":false},{"sourceId":14260969,"sourceType":"datasetVersion","datasetId":9099937}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"To run this, press \"*Runtime*\" and press \"*Run all*\" on a **free** Tesla T4 Google Colab instance!\n<div class=\"align-center\">\n<a href=\"https://unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n<a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n<a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a></a> Join Discord if you need help + ‚≠ê <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠ê\n</div>\n\nTo install Unsloth your local device, follow [our guide](https://docs.unsloth.ai/get-started/install-and-update). This notebook is licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme).\n\nYou will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)\n","metadata":{"id":"GBoEJLNUY0EJ"}},{"cell_type":"markdown","source":"### News","metadata":{"id":"CkLG6TLp-KUt"}},{"cell_type":"markdown","source":"\nIntroducing FP8 precision training for faster RL inference. [Read Blog](https://docs.unsloth.ai/new/fp8-reinforcement-learning).\n\nUnsloth's [Docker image](https://hub.docker.com/r/unsloth/unsloth) is here! Start training with no setup & environment issues. [Read our Guide](https://docs.unsloth.ai/new/how-to-train-llms-with-unsloth-and-docker).\n\n[gpt-oss RL](https://docs.unsloth.ai/new/gpt-oss-reinforcement-learning) is now supported with the fastest inference & lowest VRAM. Try our [new notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-GRPO.ipynb) which creates kernels!\n\nIntroducing [Vision](https://docs.unsloth.ai/new/vision-reinforcement-learning-vlm-rl) and [Standby](https://docs.unsloth.ai/basics/memory-efficient-rl) for RL! Train Qwen, Gemma etc. VLMs with GSPO - even faster with less VRAM.\n\nVisit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).\n","metadata":{"id":"b4SIxudc-KUt"}},{"cell_type":"markdown","source":"### Installation","metadata":{"id":"edPZXqH6-KUu"}},{"cell_type":"code","source":"%%capture\nimport os, re\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n!pip install transformers==4.56.2\n!pip install --no-deps trl==0.22.2","metadata":{"id":"05Y0VKOH-KUu","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:47:55.344606Z","iopub.execute_input":"2025-12-23T08:47:55.344927Z","iopub.status.idle":"2025-12-23T08:48:29.585993Z","shell.execute_reply.started":"2025-12-23T08:47:55.344900Z","shell.execute_reply":"2025-12-23T08:48:29.584987Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"### Unsloth","metadata":{"id":"9xLDGk41C7IF"}},{"cell_type":"code","source":"from unsloth import FastVisionModel # FastLanguageModel for LLMs\nimport torch\nfrom transformers import AutoModelForCausalLM ,AutoProcessor\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n] # More models at https://huggingface.co/unsloth\n\nmodel_path = \"unsloth/PaddleOCR-VL\"\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    model_path,\n    max_seq_length = 1024, # Choose any for long context!\n    load_in_4bit = False,     # 4bit uses much less memory\n    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n    full_finetuning=True, # We support full finetuning now!\n    auto_model=AutoModelForCausalLM,\n    trust_remote_code = True,\n    unsloth_force_compile = True,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QmUBVEnvCDJv","outputId":"6fd00662-6aed-4251-db7f-1ca987efbf80","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:48:29.588133Z","iopub.execute_input":"2025-12-23T08:48:29.588691Z","iopub.status.idle":"2025-12-23T08:49:41.872627Z","shell.execute_reply.started":"2025-12-23T08:48:29.588660Z","shell.execute_reply":"2025-12-23T08:49:41.871960Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-12-23 08:48:39.767162: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1766479720.135307      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1766479720.254305      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1766479721.071429      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766479721.071464      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766479721.071467      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1766479721.071470      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/PaddleOCR-VL:\n- configuration_paddleocr_vl.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: WARNING `trust_remote_code` is True.\nAre you certain you want to do remote code execution?\n==((====))==  Unsloth 2025.12.8: Fast Paddleocr_Vl patching. Transformers: 4.56.2.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_paddleocr_vl.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"498252e7b7cd4902b7bb8d8f2469ba68"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/PaddleOCR-VL:\n- modeling_paddleocr_vl.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f85a271ccef54be6b2598e0cdb74b792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/133 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0b685de16743eeb2f44d5b5b5a38b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b823839e83c643ea94193faa767a53a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/1.61M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ff22d549cf481cafcd8ecded283d49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13093eb5c51f4f318fde311dbc74a445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc18646a581b4b5eb99805c2c3ca8ad1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad19b978c4f4947a0080b150f324da5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6049ed635c2e4472b6eb74df3c5363c6"}},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"We now load the processor","metadata":{"id":"S5xLHboFaBd7"}},{"cell_type":"code","source":"processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)","metadata":{"id":"XXlUyieNaA7X","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:49:41.873481Z","iopub.execute_input":"2025-12-23T08:49:41.873794Z","iopub.status.idle":"2025-12-23T08:49:45.064563Z","shell.execute_reply.started":"2025-12-23T08:49:41.873754Z","shell.execute_reply":"2025-12-23T08:49:45.063960Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d91bf3ef7434fecb7e552323172ac5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processing_paddleocr_vl.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f1383d2226481ab9a66c35e71cad6a"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/unsloth/PaddleOCR-VL:\n- processing_paddleocr_vl.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa085c025cd45e8ad3100a6bab66f63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"image_processing_paddleocr_vl.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"005f7d2da25b4e8a9385a17e2b3d6a5e"}},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n\n**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!","metadata":{"id":"SXd9bTZd1aaL"}},{"cell_type":"code","source":"model = FastVisionModel.get_peft_model(\n    model,\n    r = 16,\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\",\n    random_state = 3407,\n    use_rslora = False,\n    target_modules = [\n      \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n\n\n    ]\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bZsfBuZDeCL","outputId":"7ac7afee-f076-4e64-f75b-727c8961509c","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:49:45.065430Z","iopub.execute_input":"2025-12-23T08:49:45.065690Z","iopub.status.idle":"2025-12-23T08:49:45.070082Z","shell.execute_reply.started":"2025-12-23T08:49:45.065664Z","shell.execute_reply":"2025-12-23T08:49:45.069555Z"}},"outputs":[{"name":"stdout","text":"Unsloth: Full finetuning is enabled, so .get_peft_model has no effect\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"<a name=\"Data\"></a>\n### Data Prep\nWe'll be using a sampled dataset of handwritten maths formulas. The goal is to convert these images into a computer readable form - ie in LaTeX form, so we can render it. This can be very useful for complex formulas.\n\nYou can access the dataset [here](https://huggingface.co/datasets/unsloth/LaTeX_OCR). The full dataset is [here](https://huggingface.co/datasets/linxy/LaTeX_OCR).","metadata":{"id":"vITh0KVJ10qX"}},{"cell_type":"markdown","source":"We can also render the LaTeX in the browser directly!","metadata":{"id":"NAeQ9LXCAEkW"}},{"cell_type":"markdown","source":"To format the dataset, all vision finetuning tasks should be formatted as follows:\n\n```python\n[\n{ \"role\": \"user\",\n  \"content\": [{\"type\": \"text\",  \"text\": Q}, {\"type\": \"image\", \"image\": image} ]\n},\n{ \"role\": \"assistant\",\n  \"content\": [{\"type\": \"text\",  \"text\": A} ]\n},\n]\n```","metadata":{"id":"K9CBpiISFa6C"}},{"cell_type":"markdown","source":"Let's convert the dataset into the \"correct\" format for finetuning:","metadata":{"id":"FY-9u-OD6_gE"}},{"cell_type":"markdown","source":"We look at how the conversations are structured for the first example:","metadata":{"id":"ndDUB23CGAC5"}},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom PIL import Image\nimport io\nfrom datasets import Dataset\n\n# Define your paths\nCSV_PATH =  \"/kaggle/input/medical-prescription/dataset.csv\"\nIMAGES_FOLDER =  \"/kaggle/input/medical-prescription/archive (8)/data\"\ndef load_custom_data(csv_path, images_folder):\n    df = pd.read_csv(csv_path, header=None)\n    formatted_data = []\n    print(f\"Loading data from {csv_path}...\")\n\n    for index, row in df.iterrows():\n        img_name = str(row[0])\n        ground_truth = str(row[1])\n        img_full_path = os.path.join(images_folder, img_name)\n        try:\n            image = Image.open(img_full_path).convert(\"RGB\")\n            formatted_data.append({\"image\": image, \"text\": ground_truth})\n        except Exception as e:\n            print(f\"‚ö†Ô∏è Error loading image {img_name}: {e}\")\n            continue\n    return formatted_data\n\n# 1. Load Data\ncustom_raw_data = load_custom_data(CSV_PATH, IMAGES_FOLDER)\n\n# 2. Convert to list format\ninstruction = \"OCR:\"\nconverted_dataset = []\n\nfor sample in custom_raw_data:\n    conversation = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                # Text First, then Image\n                {\"type\": \"text\", \"text\": instruction},\n                {\"type\": \"image\", \"image\": sample[\"image\"]}\n            ]\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": sample[\"text\"]}\n            ]\n        }\n    ]\n    converted_dataset.append({ \"images\": [sample[\"image\"]], \"messages\": conversation })\n\n# 3. Create Dataset\nhf_dataset = Dataset.from_list(converted_dataset)\n\n# 4. === THE CORRECTED TRANSFORM ===\ndef format_output(batch):\n    formatted_batch = {\"images\": [], \"messages\": []}\n\n    for i in range(len(batch[\"images\"])):\n        # -- Step A: Prepare Master Image --\n        raw_imgs = batch[\"images\"][i]\n        valid_images = []\n        for img in raw_imgs:\n            if isinstance(img, dict) and \"bytes\" in img:\n                valid_images.append(Image.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\"))\n            else:\n                valid_images.append(img)\n\n        formatted_batch[\"images\"].append(valid_images)\n        master_image = valid_images[0]\n\n        # -- Step B: Fix Messages --\n        raw_msgs = batch[\"messages\"][i]\n        clean_msgs = []\n\n        for msg in raw_msgs:\n            new_content = []\n            for item in msg[\"content\"]:\n                # FIX: Check if image is present AND NOT NONE\n                # The library inserts \"image\": None for text items, so we must check values.\n                if \"image\" in item and item[\"image\"] is not None:\n                    new_content.append({\"type\": \"image\", \"image\": master_image})\n                else:\n                    # It's a text item. Clean up the None keys.\n                    clean_item = {k: v for k, v in item.items() if v is not None}\n                    new_content.append(clean_item)\n\n            clean_msgs.append({\"role\": msg[\"role\"], \"content\": new_content})\n\n        formatted_batch[\"messages\"].append(clean_msgs)\n\n    return formatted_batch\n\n# Apply the transform\nhf_dataset.set_transform(format_output)\n\n# 5. Split\ndataset_split = hf_dataset.train_test_split(test_size=0.1)\ntrain_dataset = dataset_split[\"train\"]\neval_dataset= dataset_split[\"test\"]\n\n# === VERIFY OUTPUT ===\nprint(\"\\nSample Output Structure:\")\nsample = train_dataset[0]\n\n# Standard print allows you to see the true structure\nprint(sample)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:49:45.071768Z","iopub.execute_input":"2025-12-23T08:49:45.072211Z","iopub.status.idle":"2025-12-23T08:51:43.583653Z","shell.execute_reply.started":"2025-12-23T08:49:45.072186Z","shell.execute_reply":"2025-12-23T08:51:43.582881Z"}},"outputs":[{"name":"stdout","text":"Loading data from /kaggle/input/medical-prescription/dataset.csv...\n‚ö†Ô∏è Error loading image Filename: [Errno 2] No such file or directory: '/kaggle/input/medical-prescription/archive (8)/data/Filename'\n\nSample Output Structure:\n{'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1024x1024 at 0x7F99448FCDD0>], 'messages': [{'role': 'user', 'content': [{'text': 'OCR:', 'type': 'text'}, {'type': 'image', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=1024x1024 at 0x7F99448FCDD0>}]}, {'role': 'assistant', 'content': [{'text': 'Preethi Child Clinic Dr. D. Ravi Shankar Maruthi Complex, opp: South Indian bank, Suchitra. Ph: 9989102020 Timings: 9.00 AM to 10.00 am Name. 6.00 PM o 7.30 PM Shanaya M.D. (PED) PALS:MIAP Reg. No. 35400 In Case of Emergency Contact H.R. Children Hospital 10-57/A, Soubhagya Nagar, Opp: 1.D.PL Colony, Hyd Ph: 23078750 Timings: 10.00 AM to 1.00 PM 4.00 PM o 5.30 PM 8.00 PM to 10.00 PM Age montre Sex F wb.b14. Temp...........Date Cold2 con 2 du Trince idy (dy W N 2 dup wt OT-minic dryn 0.34 0.31-0.30 Anthakind dups 0.44 13 Advent dips 0.81 -0.8 -0.8 ‚ë†Nanodem manalday] 006509 –û–Ω–≥–æ–Ω —Å–ª–µ–∞—Ç tuxedonly 22/01/19 13day a', 'type': 'text'}]}]}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from transformers import TextStreamer\ndef run_inference(idx, dataset):\n    # 1. Setup Data\n    # CHANGED: \"image\" -> \"images\"[0]\n    image = dataset[idx][\"images\"][0]\n    print(image)\n    ground_truth = dataset[idx][\"messages\"][1][\"content\"][0][\"text\"]\n    instruction = \"OCR:\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n\n    # 2. Prepare Inputs\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    inputs = processor(\n        image, text_prompt, add_special_tokens=False, return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    # 3. Generate\n    print(f\"\\n--- Index {idx} ---\")\n    print(f\"Ground Truth: {ground_truth}\")\n    print(\"Prediction: \", end=\"\")\n\n    streamer = TextStreamer(processor.tokenizer, skip_prompt=True)\n    _ = model.generate(\n        **inputs,\n        streamer=streamer,\n        max_new_tokens=128,\n        use_cache=False,\n        temperature=1.5,\n        min_p=0.1\n    )\n\n# Try running it now with eval_dataset\nrun_inference(0, eval_dataset)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTlOMagwZo6z","outputId":"7103fc25-4bb3-469b-cf72-3b723c0c0ced","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:51:43.584635Z","iopub.execute_input":"2025-12-23T08:51:43.584883Z","iopub.status.idle":"2025-12-23T08:54:12.204263Z","shell.execute_reply.started":"2025-12-23T08:51:43.584858Z","shell.execute_reply":"2025-12-23T08:54:12.203576Z"}},"outputs":[{"name":"stdout","text":"<PIL.PngImagePlugin.PngImageFile image mode=RGB size=824x1280 at 0x7F99A5563BC0>\n\n--- Index 0 ---\nGround Truth: HCG The Specialt in Cancer Care Patient Name: M.R.D. No.: HealthCare Global Enterprises Ltd. #44-45/2, 2nd Cross, Raja Ram Mohan Roy Extn. (Of Lalbagh Double Road) Bangalore-560 027 Tele: +91 80 4020 6109 Fax: +91 80 2248 5962 Thimmareddy Corporate Company Name: Consultant Name: Dranean Ward: R LIPOSOMAL AMPHOTERICIN B INJ. OR. 350 mg x dalys, 4 wks. -INY AMPHOTERICIO - B. 700mg x daily x4 wis OR, INY. ISAVUCONAZOLE Name of the Doctor:: Signature of the Doctorc 200mg 1/1-1-1 x 2 weeks on capsule ISAVUCONAZOLE Loomy 1-0 days Need above medicine please help me if you know any way...\nPrediction: HealthCare Global Enterprises Ltd.\n# 44-452, 2nd Cross, Raja Ram Mohan Roy Extn. (Off. Lahargh Double Road)\nBangalore - 560 027 Tele. +91 80 4020 6109 Fax: +91 80 2248 5962\n\nPatient Name: Thummarudkar\n\nM.R.D. No.: Corporate Company Name:\n\nConsultant Name: Dr. Awanav Ward:\n\nR\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Let's first see before we do any finetuning what the model outputs for the first example!","metadata":{"id":"FecKS-dA82f5"}},{"cell_type":"code","source":"!pip install jiwer","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SxrMF3d3l312","outputId":"4e510875-b15a-4fe9-9d74-baa19f6ad45a","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:54:12.205677Z","iopub.execute_input":"2025-12-23T08:54:12.205971Z","iopub.status.idle":"2025-12-23T08:54:16.984171Z","shell.execute_reply.started":"2025-12-23T08:54:12.205936Z","shell.execute_reply":"2025-12-23T08:54:16.983405Z"}},"outputs":[{"name":"stdout","text":"Collecting jiwer\n  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\nCollecting rapidfuzz>=3.9.7 (from jiwer)\n  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\nDownloading jiwer-4.0.0-py3-none-any.whl (23 kB)\nDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"<a name=\"Train\"></a>\n### Train the model\nNow let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n\nWe use our new `UnslothVisionDataCollator` which will help in our vision finetuning setup.","metadata":{"id":"idAEIeSQ3xdS"}},{"cell_type":"code","source":"import random\nimport torch\nfrom jiwer import cer\nfrom tqdm import tqdm\n\n# 1. Setup Selection\nnum_samples = 10\nif len(train_dataset) < num_samples:\n    num_samples = len(train_dataset)\n\nindices = random.sample(range(len(train_dataset)), num_samples)\n\npredictions = []\nreferences = []\n\nprint(f\"Starting inference on {num_samples} random samples...\")\n\n# 2. Loop through samples\nfor idx in tqdm(indices):\n    image = train_dataset[idx][\"images\"][0]\n    ground_truth = train_dataset[idx][\"messages\"][1][\"content\"][0][\"text\"]\n    instruction = \"OCR:\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    inputs = processor(\n        image, text_prompt, add_special_tokens=False, return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    # Generate\n    with torch.inference_mode():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            use_cache=False,  # <--- CHANGED THIS TO FALSE\n            temperature=1.5,\n            min_p=0.1\n        )\n\n    # Decode Output\n    input_length = inputs.input_ids.shape[1]\n    generated_ids = output_ids[:, input_length:]\n    pred_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n\n    predictions.append(pred_text)\n    references.append(ground_truth)\n\n# 3. Calculate CER\nscore = cer(references, predictions)\nprint(f\"\\nResults over {num_samples} samples:\")\nprint(f\"Average CER: {score:.4f} (Lower is better)\")\nprint(f\"Percentage Accuracy: {(1-score)*100:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-23T08:54:16.985566Z","iopub.execute_input":"2025-12-23T08:54:16.985842Z","iopub.status.idle":"2025-12-23T09:09:17.240781Z","shell.execute_reply.started":"2025-12-23T08:54:16.985810Z","shell.execute_reply":"2025-12-23T09:09:17.240000Z"}},"outputs":[{"name":"stdout","text":"Starting inference on 10 random samples...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [15:00<00:00, 90.02s/it] ","output_type":"stream"},{"name":"stdout","text":"\nResults over 10 samples:\nAverage CER: 0.5980 (Lower is better)\nPercentage Accuracy: 40.20%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom unsloth.trainer import UnslothVisionDataCollator\n\nFastVisionModel.for_training(model) # Enable for training!\n\ncustom_collator = UnslothVisionDataCollator(\n    model=model,\n    processor=processor,\n    ignore_index=-100,\n    max_seq_length=1024,\n    train_on_responses_only=True,\n    instruction_part = \"User: \",\n    response_part = \"\\nAssistant:\",\n    pad_to_multiple_of = 8,\n)\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = processor.tokenizer,\n    data_collator = custom_collator,\n    train_dataset = train_dataset,\n    args = SFTConfig(\n        per_device_train_batch_size = 2,\n        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n        warmup_steps = 5,\n       \n        num_train_epochs = 3, # Set this instead of max_steps for full training runs\n        learning_rate = 5e-6,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.001,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\",\n\n        # You MUST put the below items for vision finetuning:\n        remove_unused_columns = False,\n        dataset_text_field = \"\",\n        dataset_kwargs = {\"skip_prepare_dataset\": True},\n        max_length = 1024,\n        fp16 = not torch.cuda.is_bf16_supported(),\n        bf16 = torch.cuda.is_bf16_supported(),\n    ),\n)\n","metadata":{"id":"95_Nn-89DhsL","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:09:17.241848Z","iopub.execute_input":"2025-12-23T09:09:17.242287Z","iopub.status.idle":"2025-12-23T09:09:17.604031Z","shell.execute_reply.started":"2025-12-23T09:09:17.242064Z","shell.execute_reply":"2025-12-23T09:09:17.603464Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# @title Show current memory stats\ngpu_stats = torch.cuda.get_device_properties(0)\nstart_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nmax_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\nprint(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\nprint(f\"{start_gpu_memory} GB of memory reserved.\")","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"id":"2ejIt2xSNKKp","outputId":"b656d975-a1db-4e00-8ced-969702a36bc6","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:09:17.604918Z","iopub.execute_input":"2025-12-23T09:09:17.605227Z","iopub.status.idle":"2025-12-23T09:09:17.610203Z","shell.execute_reply.started":"2025-12-23T09:09:17.605192Z","shell.execute_reply":"2025-12-23T09:09:17.609619Z"}},"outputs":[{"name":"stdout","text":"GPU = Tesla T4. Max memory = 14.741 GB.\n6.512 GB of memory reserved.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"trainer_stats = trainer.train()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"yqxqAZ7KJ4oL","outputId":"c7727fcc-08fa-402c-d0dc-daedcf3ca4ad","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:09:17.611021Z","iopub.execute_input":"2025-12-23T09:09:17.611269Z","iopub.status.idle":"2025-12-23T09:13:57.247664Z","shell.execute_reply.started":"2025-12-23T09:09:17.611230Z","shell.execute_reply":"2025-12-23T09:13:57.247030Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 116 | Num Epochs = 3 | Total steps = 87\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n \"-____-\"     Trainable parameters = 958,588,736 of 958,588,736 (100.00% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='87' max='87' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [87/87 04:30, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>5.051900</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>6.575800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.632100</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4.887600</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>4.952900</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>4.397100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>6.871000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>7.505600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>5.115800</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>5.543000</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>5.493300</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>4.636900</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>6.370700</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>5.163200</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>3.949200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.075800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>5.347300</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>4.100700</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.576400</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>5.021400</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>6.998700</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>4.523800</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>6.201800</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.679400</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>4.299300</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>4.209400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>5.921300</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>5.640800</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>4.705600</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>3.593600</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>5.298900</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>7.080000</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>5.428900</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>3.282900</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>2.963000</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>2.025500</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>6.117600</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>4.259700</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>4.545300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.471500</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>5.324000</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>4.940200</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>4.089000</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>3.308200</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>4.011900</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>6.725400</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>3.325600</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>1.681300</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>2.569400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.936400</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>5.147300</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>3.931300</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>4.460400</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>6.049400</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>2.968200</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>3.251500</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>3.457400</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>7.053100</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>4.163200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.691600</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>3.642300</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>6.773900</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>2.609900</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>5.249500</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>2.111000</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>4.335500</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>2.547800</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>7.907800</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>3.151200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>2.507200</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>2.686200</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>5.199700</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>4.562300</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>3.959200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>3.892200</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>2.606200</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>2.867500</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>2.182900</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>3.242700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.509900</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>2.635000</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>3.354800</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>1.561000</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>3.740800</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>5.465200</td>\n    </tr>\n    <tr>\n      <td>86</td>\n      <td>2.922900</td>\n    </tr>\n    <tr>\n      <td>87</td>\n      <td>4.517800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# @title Show final memory and time stats\nused_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\nused_memory_for_lora = round(used_memory - start_gpu_memory, 3)\nused_percentage = round(used_memory / max_memory * 100, 3)\nlora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\nprint(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\nprint(\n    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n)\nprint(f\"Peak reserved memory = {used_memory} GB.\")\nprint(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\nprint(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\nprint(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")","metadata":{"cellView":"form","id":"pCqnaKmlO1U9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a1c2288-619a-4f8f-a522-863682536bf4","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:13:57.248931Z","iopub.execute_input":"2025-12-23T09:13:57.249189Z","iopub.status.idle":"2025-12-23T09:13:57.255315Z","shell.execute_reply.started":"2025-12-23T09:13:57.249158Z","shell.execute_reply":"2025-12-23T09:13:57.254698Z"}},"outputs":[{"name":"stdout","text":"277.3698 seconds used for training.\n4.62 minutes used for training.\nPeak reserved memory = 13.547 GB.\nPeak reserved memory for training = 7.035 GB.\nPeak reserved memory % of max memory = 91.9 %.\nPeak reserved memory for training % of max memory = 47.724 %.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"<a name=\"Inference\"></a>\n### Inference\nLet's run the model! You can change the instruction and input - leave the output blank!\n\nWe use `min_p = 0.1` and `temperature = 1.5`. Read this [Tweet](https://x.com/menhguin/status/1826132708508213629) for more information on why.","metadata":{"id":"ekOmTR1hSNcr"}},{"cell_type":"code","source":"import random\nimport torch\nfrom jiwer import cer\nfrom tqdm import tqdm\n\n# 1. Setup Selection\nnum_samples = 10\nif len(train_dataset) < num_samples:\n    num_samples = len(train_dataset)\n\nindices = random.sample(range(len(train_dataset)), num_samples)\n\npredictions = []\nreferences = []\n\nprint(f\"Starting inference on {num_samples} random samples...\")\n\n# 2. Loop through samples\nfor idx in tqdm(indices):\n    image = train_dataset[idx][\"images\"][0]\n    ground_truth = train_dataset[idx][\"messages\"][1][\"content\"][0][\"text\"]\n    instruction = \"OCR:\"\n\n    messages = [\n        {\"role\": \"user\", \"content\": [\n            {\"type\": \"image\"},\n            {\"type\": \"text\", \"text\": instruction}\n        ]}\n    ]\n\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    inputs = processor(\n        image, text_prompt, add_special_tokens=False, return_tensors=\"pt\"\n    ).to(\"cuda\")\n\n    # Generate\n    with torch.inference_mode():\n        output_ids = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            use_cache=False,  # <--- CHANGED THIS TO FALSE\n            temperature=1.5,\n            min_p=0.1\n        )\n\n    # Decode Output\n    input_length = inputs.input_ids.shape[1]\n    generated_ids = output_ids[:, input_length:]\n    pred_text = processor.decode(generated_ids[0], skip_special_tokens=True)\n\n    predictions.append(pred_text)\n    references.append(ground_truth)\n\n# 3. Calculate CER\nscore = cer(references, predictions)\nprint(f\"\\nResults over {num_samples} samples:\")\nprint(f\"Average CER: {score:.4f} (Lower is better)\")\nprint(f\"Percentage Accuracy: {(1-score)*100:.2f}%\")","metadata":{"id":"fpwhtfsSnENd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f170aeca-ffd4-43b9-8d86-6143164277b0","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T10:06:28.106822Z","iopub.execute_input":"2025-12-23T10:06:28.107149Z","iopub.status.idle":"2025-12-23T10:23:00.147666Z","shell.execute_reply.started":"2025-12-23T10:06:28.107119Z","shell.execute_reply":"2025-12-23T10:23:00.147001Z"}},"outputs":[{"name":"stdout","text":"Starting inference on 10 random samples...\n","output_type":"stream"},{"name":"stderr","text":"100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [16:32<00:00, 99.20s/it]","output_type":"stream"},{"name":"stdout","text":"\nResults over 10 samples:\nAverage CER: 0.4050 (Lower is better)\nPercentage Accuracy: 59.50%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"<a name=\"Save\"></a>\n### Saving, loading finetuned models\nTo save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n\n**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!","metadata":{"id":"uMuVrWbjAzhc"}},{"cell_type":"code","source":"model.save_pretrained(\"lora_model_v3\")  # Local saving\ntokenizer.save_pretrained(\"lora_model_v3\")\n# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving","metadata":{"id":"upcOlWe7A1vc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5a4975a1-6f9b-4dcb-f76c-1a189ecbdd7e","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:40:20.558700Z","iopub.execute_input":"2025-12-23T09:40:20.558946Z","iopub.status.idle":"2025-12-23T09:40:27.923326Z","shell.execute_reply.started":"2025-12-23T09:40:20.558919Z","shell.execute_reply":"2025-12-23T09:40:27.922580Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('lora_model_v3/tokenizer_config.json',\n 'lora_model_v3/special_tokens_map.json',\n 'lora_model_v3/chat_template.jinja',\n 'lora_model_v3/tokenizer.model',\n 'lora_model_v3/added_tokens.json',\n 'lora_model_v3/tokenizer.json')"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:","metadata":{"id":"AEEcJ4qfC7Lp"}},{"cell_type":"markdown","source":"### Saving to float16 for VLLM\n\nWe also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens.","metadata":{"id":"f422JgM9sdVT"}},{"cell_type":"code","source":"# Select ONLY 1 to save! (Both not needed!)\n\n# Save locally to 16bit\nif False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n\n# To export and save to your Hugging Face account\nif False: model.push_to_hub_merged(\"manojemjay/unsloth_finetune\", tokenizer, token = \"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\")","metadata":{"id":"iHjt_SMYsd3P","trusted":true,"execution":{"iopub.status.busy":"2025-12-23T09:49:25.321416Z","iopub.status.idle":"2025-12-23T09:49:25.321716Z","shell.execute_reply.started":"2025-12-23T09:49:25.321584Z","shell.execute_reply":"2025-12-23T09:49:25.321604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n\nSome other links:\n1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)\n2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)\n3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)\n6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!\n\n<div class=\"align-center\">\n  <a href=\"https://unsloth.ai\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n  <a href=\"https://discord.gg/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord.png\" width=\"145\"></a>\n  <a href=\"https://docs.unsloth.ai/\"><img src=\"https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true\" width=\"125\"></a>\n\n  Join Discord if you need help + ‚≠êÔ∏è <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> ‚≠êÔ∏è\n\n  This notebook and all Unsloth notebooks are licensed [LGPL-3.0](https://github.com/unslothai/notebooks?tab=LGPL-3.0-1-ov-file#readme)\n</div>\n","metadata":{"id":"ORCxqLRwY0EZ"}}]}
